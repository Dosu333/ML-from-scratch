# ğŸ§® Machine Learning Algorithms from Scratch (NumPy)

This repository is a collection of **machine learning algorithms implemented entirely from scratch using NumPy** â€” no Scikit-Learn, no TensorFlow.  
Each implementation focuses on understanding **how the math works under the hood**, with visualizations, derivations, and performance evaluation.

---

## ğŸ“š Algorithms Implemented

### 1ï¸âƒ£ [Linear Regression](./linear_regression/)
**Goal:** Predict continuous outcomes by minimizing Mean Squared Error (MSE).

**Highlights:**
- Built **from scratch** using gradient descent.  
- Includes **cost function derivation**, learning rate tuning, and convergence visualization.  
- Evaluated with **RÂ² score** and **MSE**.  
- Visualizes regression line fit vs. data.

ğŸ“˜ [Read full details â†’](./linear_regression/readme.md)

---

### 2ï¸âƒ£ [Logistic Regression](./logistic_regression/)
**Goal:** Classify data points into binary categories using the sigmoid function.

**Highlights:**
- Implements **binary cross-entropy loss** and **L2 regularization**.  
- Uses **gradient descent** for optimization.  
- Visualizes **decision boundary**, **sigmoid function**, and **cost convergence**.  
- Evaluated with **Precision** and **Recall** metrics.

ğŸ“˜ [Read full details â†’](./logistic_regression/readme.md)

---

## ğŸ§© Repo Structure

```

ğŸ“‚ ML-ALGO
â”£ ğŸ“ Linear_Regression
â”ƒ â”£ ğŸ“œ linear_regression.ipynb
â”ƒ â”— ğŸ“œ README.md
â”£ ğŸ“ Logistic_Regression
â”ƒ â”£ ğŸ“œ logistic_regression.ipynb
â”ƒ â”— ğŸ“œ README.md
â”£ ğŸ“œ requirements.txt
â”— ğŸ“œ README.md   â† (this file)

```

---

## ğŸ› ï¸ Technologies Used
- **Python 3.11+**
- **NumPy** â€” numerical operations  
- **Matplotlib** â€” plotting  
- **Seaborn / Pandas** â€” data visualization  
- *(No machine learning frameworks used)*  

---

## ğŸ“Š Visuals Included
Each algorithm notebook includes:
- Loss/cost function convergence plots  
- Model fit visualization  
- Sigmoid / Decision boundary plots (for logistic regression)

---

## ğŸ”¬ Learning Outcomes
By exploring these implementations, youâ€™ll learn:
- How gradient descent works step by step  
- How to derive cost and gradient functions  
- How to visualize convergence and model performance  
- How regularization affects model training  

---

## ğŸŒ± Next Steps
Upcoming algorithms to be added:
- [ ] Decision Tree
- [ ] K-Means 
- [ ] PCA  


---

## ğŸ§‘â€ğŸ’» Author
**Oladosu Larinde**  
Lead Software Engineer
ğŸ’¡ Passionate about building intelligent systems and teaching others how they work.

---

## ğŸŒŸ Inspiration
This project was inspired by:
- Andrew Ngâ€™s ML Course (Coursera)  
- â€œHands-On Machine Learning with Scikit-Learn, Keras & TensorFlowâ€ by AurÃ©lien GÃ©ron
