# ðŸ§  Logistic Regression from Scratch (with L2 Regularization)

This project implements **Logistic Regression** from scratch using **Gradient Descent** optimization and optional **L2 Regularization** (Ridge). It demonstrates how a linear model can perform binary classification by learning probabilities through the **sigmoid function**.

---

## ðŸ§® Mathematical Background

### 1. Logistic Model (Hypothesis Function)

Logistic regression predicts the probability that a sample belongs to class 1:

```
Å· = Ïƒ(z) = 1 / (1 + exp(-z))
```

where:

```
z = wáµ€x + b
```

* Å· â€” predicted probability of class 1
* w â€” weight vector
* b â€” bias term
* x â€” feature vector
* Ïƒ(z) â€” sigmoid function mapping real numbers to (0, 1)

---

### 2. Decision Rule

The predicted class is determined by thresholding the probability:

```
if Å· >= 0.5 â†’ predict 1
else â†’ predict 0
```

You can adjust the threshold for precision/recall trade-off.

---

### 3. Cost Function (Binary Cross-Entropy)

The cost function measures how far the predictions are from the true labels:

```
J(w, b) = -(1/m) * Î£ [ yáµ¢ * log(Å·áµ¢) + (1 - yáµ¢) * log(1 - Å·áµ¢) ]
```

where:

* m = number of training samples
* yáµ¢ = true label (0 or 1)
* Å·áµ¢ = predicted probability

---

### 4. L2 Regularization (Ridge)

To prevent overfitting, we add an L2 penalty term that discourages large weights:

```
J_reg(w, b) = J(w, b) + (Î» / (2m)) * Î£ wâ±¼Â²
```

where:

* Î» (lambda) controls the regularization strength (higher Î» â†’ stronger penalty).

---

### 5. Gradient Descent Optimization

To minimize the cost, gradients are computed and used to update parameters:

```
âˆ‚J/âˆ‚wâ±¼ = (1/m) * Î£((Å·áµ¢ - yáµ¢) * xâ±¼áµ¢) + (Î»/m) * wâ±¼
âˆ‚J/âˆ‚b  = (1/m) * Î£(Å·áµ¢ - yáµ¢)
```

Parameter updates:

```
wâ±¼ := wâ±¼ - Î± * (âˆ‚J/âˆ‚wâ±¼)
b  := b  - Î± * (âˆ‚J/âˆ‚b)
```

where:

* Î± = learning rate
* The loop continues until cost convergence or reaching max iterations.

---

### 6. Convergence Criterion

Training stops when the cost no longer changes significantly:

```
|J(t) - J(t-1)| â‰¤ Îµ
```

---

## âš™ï¸ Implementation Details

### Parameters

| Parameter       | Description                            | Default |
| --------------- | -------------------------------------- | ------- |
| `learning_rate` | Step size for gradient descent updates | 0.5     |
| `epsi1on`       | Convergence threshold                  | 1e-6    |
| `_lambda`       | Regularization parameter               | 0       |
| `max_iter`      | Maximum number of iterations           | 10,000  |

---

## ðŸ§© Class Overview

```python
class LogisticRegression:
    def __init__(self, learning_rate=0.5, epsi1on=1e-6, _lambda=0, max_iter=10000)
```

### Methods

#### `fit(X, y)`

Trains the model by iteratively:

1. Computing predictions Å· = Ïƒ(Xw + b)
2. Evaluating cost using the binary cross-entropy loss
3. Computing gradients for each parameter
4. Updating weights and bias until convergence

#### `_cost_function(X, y, w, b)`

Calculates the cost (and includes the regularization term if `_lambda` > 0).

#### `predict_proba(X)`

Returns predicted probabilities between 0 and 1.

#### `predict(X, threshold=0.5)`

Returns binary class predictions (0 or 1) based on a probability threshold.

---

## ðŸ§  Step-by-Step Visualization

The project first visualizes:

1. The **feature space** with colors based on target classes (`sns.pairplot`).
2. The **predicted probabilities** using a color map to show how the sigmoid function separates classes.

---

## ðŸ“Š Example Usage

```python
import numpy as np

np.random.seed(42)
X = np.random.rand(100, 2)
y = np.random.randint(0, 2, size=(100,))

model = LogisticRegression(_lambda=0.1)
model.fit(X, y)

# Predictions
y_pred_proba = model.predict_proba(X)
y_pred = model.predict(X, threshold=0.5)

print("Predicted probabilities:", y_pred_proba[:5])
print("Predicted classes:", y_pred[:5])
```

---

## ðŸ§¾ Model Evaluation

We can compute standard binary classification metrics:

```
True Positive  (TP): predicted 1, actual 1
True Negative  (TN): predicted 0, actual 0
False Positive (FP): predicted 1, actual 0
False Negative (FN): predicted 0, actual 1
```

From these:

```
Precision = TP / (TP + FP)
Recall    = TP / (TP + FN)
```

These metrics describe how well the model distinguishes the two classes.

---

## ðŸ“ˆ Visualization Example

**Predicted probabilities heatmap:**

```python
plt.scatter(X[:, 0], X[:, 1], c=y_pred_proba, cmap='viridis')
plt.colorbar(label='Predicted Probability')
plt.xlabel('feature_0')
plt.ylabel('feature_1')
plt.title('Predicted Probabilities from Logistic Regression')
plt.show()
```

**Cost function convergence:**

```python
plt.plot(cost_history)
plt.xlabel("Iteration")
plt.ylabel("Cost")
plt.title("Cost Function Convergence")
plt.show()
```

---

## ðŸ“¦ Dependencies

```bash
pip install numpy seaborn matplotlib pandas
```

---

## ðŸ§© Key Concepts Summary

| Concept          | Formula                                     |
| ---------------- | ------------------------------------------- |
| Sigmoid Function | Ïƒ(z) = 1 / (1 + exp(-z))                    |
| Hypothesis       | Å· = Ïƒ(wáµ€x + b)                              |
| Cost Function    | J = -(1/m) Î£[y log(Å·) + (1 - y) log(1 - Å·)] |
| Regularization   | (Î» / 2m) Î£wÂ²                                |
| Gradient (w)     | (1/m) Î£((Å· - y)x) + (Î»/m)w                  |
| Gradient (b)     | (1/m) Î£(Å· - y)                              |
